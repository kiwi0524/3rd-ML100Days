{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能聽過 XGBoost/Light-GBM，這些都是資料科學競賽中最常用的機器學習模型，但其實這些演算法背後原理都是基於 Gradient-boosting 進而優化，強烈建議您對本日的課程與補充教材多花點時間閱讀與理解。 ",
    "核心概念就是透過計算梯度，來讓下一棵生成的樹能夠根據梯度方向，試圖讓 Loss 變得更小！\n",
    "由完整的 Ensemble 概念 by 李宏毅教授影片可了解\n",
    "1.bagging的目的在降低varience, 用於容易overfit複雜的演算法上\n",
    "例如decision tree分支越多，可得到100%的正確率，但也容易overfitting\n",
    "2.boosting的目的在提升varience，用於很弱的演算法上\n",
    "對於很弱的演算法，例如decision stump，可以用不同但互補的function，用有順序的訓練方式，調整每次的權重，而得到100%的正確率\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
